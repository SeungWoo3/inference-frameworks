{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'onnxruntime'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 프레임워크별 라이브러리 임포트\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m  \u001b[38;5;66;03m# PyTorch 사용 시\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01monnxruntime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mort\u001b[39;00m  \u001b[38;5;66;03m# ONNX 사용 시\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 텐서RT 관련 라이브러리 (실제 사용 시 추가 설정 필요)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorrt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'onnxruntime'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 프레임워크별 라이브러리 임포트\n",
    "import torch  # PyTorch 사용 시\n",
    "# import onnxruntime as ort  # ONNX 사용 시\n",
    "\n",
    "# 텐서RT 관련 라이브러리 (실제 사용 시 추가 설정 필요)\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 변수: 모델과 프레임워크 선택\n",
    "config = {\n",
    "    \"model\": \"simple\",  # 옵션: \"simple\", \"vit\", \"trajectory_prediction\"\n",
    "    \"framework\": \"pytorch\",  # 옵션: \"pytorch\", \"onnx\", \"tensorrt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config):\n",
    "    \"\"\"\n",
    "    모델 파라미터 파일은 소스코드 폴더 내의 \"models\" 폴더에 저장되어 있다고 가정합니다.\n",
    "    파일명은 예를 들어 \"simple_model.pt\", \"vit_model.onnx\", \"trajectory_prediction_model.trt\" 식으로 저장합니다.\n",
    "    \"\"\"\n",
    "    model_type = config[\"model\"]\n",
    "    framework = config[\"framework\"]\n",
    "    model_dir = \"models\"\n",
    "    \n",
    "    if framework == \"pytorch\":\n",
    "        model_path = os.path.join(model_dir, f\"{model_type}_model.pt\")\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"PyTorch 모델 파일을 찾을 수 없습니다: {model_path}\")\n",
    "        model = torch.load(model_path, map_location=torch.device(\"cpu\"))\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    elif framework == \"onnx\":\n",
    "        model_path = os.path.join(model_dir, f\"{model_type}_model.onnx\")\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"ONNX 모델 파일을 찾을 수 없습니다: {model_path}\")\n",
    "        session = ort.InferenceSession(model_path)\n",
    "        return session\n",
    "\n",
    "    elif framework == \"tensorrt\":\n",
    "        model_path = os.path.join(model_dir, f\"{model_type}_model.trt\")\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"TensorRT 모델 파일을 찾을 수 없습니다: {model_path}\")\n",
    "        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "        runtime = trt.Runtime(TRT_LOGGER)\n",
    "        with open(model_path, \"rb\") as f:\n",
    "            engine_data = f.read()\n",
    "        engine = runtime.deserialize_cuda_engine(engine_data)\n",
    "        return engine\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"지원하지 않는 프레임워크입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(config):\n",
    "    \"\"\"\n",
    "    모델 종류에 맞는 입력 데이터를 생성합니다.\n",
    "    각 모델은 예시로 랜덤 데이터를 사용하며,\n",
    "    실제로는 모델이 기대하는 입력 형상과 데이터를 맞춰야 합니다.\n",
    "    \"\"\"\n",
    "    model_type = config[\"model\"]\n",
    "\n",
    "    if model_type == \"simple\":\n",
    "        # 예: 간단한 피드포워드 모델이 [1, 10] shape의 입력을 요구한다고 가정\n",
    "        input_data = np.random.rand(1, 10).astype(np.float32)\n",
    "    elif model_type == \"vit\":\n",
    "        # 예: Vision Transformer가 [1, 3, 224, 224] shape의 이미지를 요구한다고 가정\n",
    "        input_data = np.random.rand(1, 3, 224, 224).astype(np.float32)\n",
    "    elif model_type == \"trajectory_prediction\":\n",
    "        # 예: 궤적 예측 모델이 [1, 20, 2] shape의 시퀀스 데이터를 요구한다고 가정\n",
    "        input_data = np.random.rand(1, 20, 2).astype(np.float32)\n",
    "    else:\n",
    "        raise ValueError(\"알 수 없는 모델 타입입니다.\")\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(config, model, input_data):\n",
    "    \"\"\"\n",
    "    선택한 프레임워크에 따라 추론을 수행합니다.\n",
    "    각 프레임워크에 맞는 전처리 및 실행 코드를 포함합니다.\n",
    "    \"\"\"\n",
    "    framework = config[\"framework\"]\n",
    "\n",
    "    if framework == \"pytorch\":\n",
    "        # PyTorch 추론\n",
    "        with torch.no_grad():\n",
    "            # numpy 데이터를 tensor로 변환\n",
    "            input_tensor = torch.from_numpy(input_data)\n",
    "            output = model(input_tensor)\n",
    "            return output.detach().cpu().numpy()\n",
    "\n",
    "    elif framework == \"onnx\":\n",
    "        # ONNX 추론: 입력 이름을 가져와서 실행\n",
    "        input_name = model.get_inputs()[0].name\n",
    "        output = model.run(None, {input_name: input_data})\n",
    "        return output\n",
    "\n",
    "    elif framework == \"tensorrt\":\n",
    "        # TensorRT 추론은 복잡합니다.\n",
    "        # 아래는 매우 단순화한 예시로, 실제 사용 시 GPU 버퍼 할당 및 동기화 과정이 필요합니다.\n",
    "        context = model.create_execution_context()\n",
    "\n",
    "        # 입력 및 출력 버퍼 할당 (예시)\n",
    "        input_shape = input_data.shape\n",
    "        d_input = cuda.mem_alloc(input_data.nbytes)\n",
    "        # 여기서는 출력의 shape와 크기를 미리 알아야 합니다.\n",
    "        output_shape = (1, )  # 실제 모델에 맞게 수정 필요\n",
    "        output_size = np.prod(output_shape) * np.float32().itemsize\n",
    "        d_output = cuda.mem_alloc(output_size)\n",
    "\n",
    "        # 입력 데이터 GPU로 복사\n",
    "        cuda.memcpy_htod(d_input, input_data)\n",
    "\n",
    "        # 실행 (바인딩 인덱스 0: 입력, 1: 출력라고 가정)\n",
    "        bindings = [int(d_input), int(d_output)]\n",
    "        context.execute_v2(bindings)\n",
    "\n",
    "        # 결과를 CPU로 복사\n",
    "        output = np.empty(output_shape, dtype=np.float32)\n",
    "        cuda.memcpy_dtoh(output, d_output)\n",
    "        return output\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"지원하지 않는 프레임워크입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInference output:\u001b[39m\u001b[33m\"\u001b[39m, output)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# 모델 로드\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     model = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# 모델에 맞는 입력 데이터 생성\u001b[39;00m\n\u001b[32m      5\u001b[39m     input_data = get_input(config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m      8\u001b[39m model_dir = \u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m framework == \u001b[33m\"\u001b[39m\u001b[33mpytorch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     model_path = \u001b[43mos\u001b[49m.path.join(model_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_model.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(model_path):\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPyTorch 모델 파일을 찾을 수 없습니다: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 모델 로드\n",
    "    model = load_model(config)\n",
    "    # 모델에 맞는 입력 데이터 생성\n",
    "    input_data = get_input(config)\n",
    "    # 추론 실행\n",
    "    output = run_inference(config, model, input_data)\n",
    "    print(\"Inference output:\", output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
